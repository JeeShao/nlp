 <title_NN> Representation_NNP of_IN Linguistic_NNP Form_NNP and_CC Function_NNP in_IN Recurrent_NNP Neural_NNP Networks_NNP </title_NNP>
 <author_NN> ¨¢kos_NNP K¨¢d¨¢r_NNP, Grzegorz_NNP Chrupala_NNP £¬_NNP Afra_NNP Alishahi_NNP </author_NNP>



 <abstract_JJ> We_PRP present_VBP novel_JJ methods_NNS for_IN analyzing_VBG the_DT activation_NN patterns_NNS of_IN recurrent_JJ neural_JJ networks_NNS from_IN a_DT linguistic_JJ point_NN of_IN view_NN and_CC explore_VB the_DT types_NNS of_IN linguistic_JJ structure_NN they_PRP learn_VBP. As_IN a_DT case_NN study_NN, we_PRP use_VBP a_DT standard_NN standalone_NN language_NN model_NN, and_CC a_DT multi-task_NN gated_VBN recurrent_NN network_NN architecture_NN consisting_VBG of_IN two_CD parallel_JJ pathways_NNS with_IN shared_VBN word_NN embeddings_NNS: The_DT Visual_NNP pathway_NN is_VBZ trained_VBN on_IN predicting_VBG the_DT representations_NNS of_IN the_DT visual_JJ scene_NN corresponding_VBG to_TO an_DT input_NN sentence_NN, and_CC the_DT Textual_NNP pathway_NN is_VBZ trained_VBN to_TO predict_VB the_DT next_JJ word_NN in_IN the_DT same_JJ sentence_NN. We_PRP propose_VBP a_DT method_NN for_IN estimating_VBG the_DT amount_NN of_IN contribution_NN of_IN individual_JJ tokens_NNS in_IN the_DT input_NN to_TO the_DT final_JJ prediction_NN of_IN the_DT networks_NNS. Using_VBG this_DT method_NN, we_PRP show_VBP that_IN the_DT Visual_NNP pathway_NN pays_NNS selective_JJ attention_NN to_TO lexical_JJ categories_NNS and_CC grammatical_JJ functions_NNS that_WDT carry_VBP semantic_JJ information_NN, and_CC learns_VBZ to_TO treat_VB word_NN types_NNS differently_RB depending_VBG on_IN their_PRP$ grammatical_JJ function_NN and_CC their_PRP$ position_NN in_IN the_DT sequential_JJ structure_NN of_IN the_DT sentence_NN. In_IN contrast_NN, the_DT language_NN models_NNS are_VBP comparatively_RB more_RBR sensitive_JJ to_TO words_NNS with_IN a_DT syntactic_JJ function_NN. Further_JJ analysis_NN of_IN the_DT most_RBS informative_JJ n-gram_JJ contexts_NN for_IN each_DT model_NN shows_VBZ that_IN in_IN comparison_NN with_IN the_DT Visual_NNP pathway_NN, the_DT language_NN models_NNS react_VBP more_JJR strongly_RB to_TO abstract_VB contexts_NN that_WDT represent_VBP syntactic_JJ constructions_NNS. </abstract_NN>



 <Introduction_NNP> Recurrent_NNP neural_JJ networks_NNS (RNNs_NNP) were_VBD introduced_VBN by_IN Elman_NNP (1990_CD) as_IN a_DT connectionist_NN architecture_NN with_IN the_DT ability_NN to_TO model_VB the_DT temporal_JJ dimension_NN. They_PRP have_VBP proved_VBN popular_JJ for_IN modeling_VBG language_NN data_NNS as_IN they_PRP learn_VBP representations_NNS of_IN words_NNS and_CC larger_JJR linguistic_JJ units_NNS directly_RB from_IN the_DT input_NN data_NNS, without_IN feature_NN engineering_NN. Variations_NNS of_IN the_DT RNN_NNP architecture_NN have_VBP been_VBN applied_VBN in_IN several_JJ NLP_NNP domains_NNS such_JJ as_IN parsing_NN (Vinyals_NNP et_RB al._RB, 2015_CD) and_CC machine_NN translation_NN (Bahdanau_NNP, Cho_NNP, and_CC Bengio_NNP, 2015_CD), as_RB well_RB as_IN in_IN computer_NN vision_NN applications_NNS such_JJ as_IN image_NN generation_NN (Gregor_NNP et_RB al._RB, 2015_CD) and_CC object_JJ segmentation_NN (Visin_NNP et_RB al._RB, 2016_CD). RNNs_NNP are_VBP also_RB important_JJ components_NNS of_IN systems_NNS integrating_VBG vision_NN and_CC language¡ªfor_JJ example_NN, image_NN (Karpathy_NNP and_CC Fei-Fei_NNP, 2015_CD) and_CC video_$ captioning_NN (Yu_NNP et_RB al._RB, 2015_CD).
 These_DT networks_NNS can_MD represent_VB variable-length_JJ linguistic_JJ expressions_NNS by_IN encoding_VBG them_PRP into_IN a_DT fixed-size_JJ low-dimensional_JJ vector_NN. The_DT nature_NN and_CC the_DT role_NN of_IN the_DT components_NNS of_IN these_DT representations_NNS are_VBP not_RB directly_RB interpretable_JJ as_IN they_PRP are_VBP a_DT complex_JJ, non-linear_JJ function_NN of_IN the_DT input_NN. There_EX have_VBP recently_RB been_VBN numerous_JJ efforts_NNS to_TO visualize_VB deep_JJ models_NNS such_JJ as_IN convolutional_JJ neural_JJ networks_NNS in_IN the_DT domain_NN of_IN computer_NN vision_NN, but_CC much_RB less_JJR so_RB for_IN variants_NNS of_IN RNNs_NNP and_CC for_IN language_NN processing_NN.
 The_DT present_JJ article_NN develops_VBZ novel_JJ methods_NNS for_IN uncovering_VBG abstract_JJ linguistic_JJ knowledge_NN encoded_VBN by_IN the_DT distributed_JJ representations_NNS of_IN RNNs_NNP, with_IN a_DT specific_JJ focus_NN on_IN analyzing_VBG the_DT hidden_JJ activation_NN patterns_VBZ rather_RB than_IN word_NN embeddings_NNS and_CC on_IN the_DT syntactic_JJ generalizations_NNS that_WDT models_NNS learn_VBP to_TO capture_VB. In_IN the_DT current_JJ work_NN we_PRP apply_VBP our_PRP$ methods_NNS to_TO a_DT specific_JJ architecture_NN trained_VBN on_IN specific_JJ tasks_NNS, but_CC also_RB provide_VBP pointers_NNS about_IN how_WRB to_TO generalize_VB the_DT proposed_VBN analysis_NN to_TO other_JJ settings_NNS.
 As_IN our_PRP$ case_NN study_NN we_PRP picked_VBD the_DT Imaginet_NNP model_NN introduced_VBN by_IN Chrupa_NNP a_DT, K¨¢d¨¢r_NNP, and_CC Alishahi_NNP (2015_CD). It_PRP is_VBZ a_DT multi-task_JJ, multi-modal_JJ architecture_NN consisting_VBG of_IN two_CD gated-recurrent_JJ unit_NN (GRU_NNP) (Cho_NNP et_RB al._RB, 2014_CD; Chung_NNP et_FW al._NN, 2014_CD) pathways_NNS and_CC a_DT shared_VBN word_NN embedding_VBG matrix_NN. One_CD of_IN the_DT GRUs_NNP (Visual_NNP) is_VBZ trained_VBN to_TO predict_VB image_NN vectors_NNS given_VBN image_NN descriptions_NNS, and_CC the_DT other_JJ pathway_NN (Textual_NNP) is_VBZ a_DT language_NN model_NN, trained_VBN to_TO sequentially_RB predict_VB each_DT word_NN in_IN the_DT descriptions_NNS. This_DT particular_JJ architecture_NN allows_VBZ a_DT comparative_JJ analysis_NN of_IN the_DT hidden_JJ activation_NN patterns_NNS between_IN networks_NNS trained_VBN on_IN two_CD different_JJ tasks_NNS, while_IN keeping_VBG the_DT training_NN data_NNS and_CC the_DT word_NN embeddings_VBZ fixed_VBN. Recurrent_JJ neural_JJ language_NN models_NNS akin_VBP to_TO Textual_NNP, which_WDT are_VBP trained_VBN to_TO predict_VB the_DT next_JJ symbol_NN in_IN a_DT sequence_NN, are_VBP relatively_RB well_RB understood_JJ, and_CC there_RB have_VBP been_VBN some_DT attempts_NNS to_TO analyze_VB their_PRP$ internal_JJ states_NNS (Elman_NNP, 1991_CD; Karpathy_NNP, Johnson_NNP, and_CC Li_NNP, 2016_CD, among_IN others_NNS). In_IN constrast_NN, Visual_NNP maps_VBZ a_DT complete_JJ sequence_NN of_IN words_NNS to_TO a_DT representation_NN of_IN a_DT corresponding_JJ visual_JJ scene_NN and_CC is_VBZ a_DT less_RBR commonly_RB encountered_VBN, but_CC more_JJR interesting_JJ, model_NN from_IN the_DT point_NN of_IN view_NN of_IN representing_VBG meaning_VBG conveyed_JJ via_IN linguistic_JJ structure_NN. For_IN comparison_NN, we_PRP also_RB consider_VBP a_DT standard_NN standalone_NN language_NN model_NN.
 We_PRP report_VBP a_DT thorough_JJ quantitative_JJ analysis_NN to_TO provide_VB a_DT linguistic_JJ interpretation_NN of_IN the_DT networks_NNS '_POS activation_NN patterns_NNS. We_PRP present_VBP a_DT series_NN of_IN experiments_NNS using_VBG a_DT novel_JJ method_NN we_PRP call_VBP omission_JJ score_NN to_TO measure_VB the_DT importance_NN of_IN input_NN tokens_NNS to_TO the_DT final_JJ prediction_NN of_IN models_NNS that_IN compute_NN distributed_VBD representations_NNS of_IN sentences_NNS. Furthermore_RB, we_PRP introduce_VBP a_DT more_RBR global_JJ measure_NN for_IN estimating_VBG the_DT informativeness_NN of_IN various_JJ types_NNS of_IN n-gram_JJ contexts_NN for_IN each_DT model_NN. These_DT techniques_NNS can_MD be_VB applied_VBN to_TO various_JJ RNN_NNP architectures_NNS such_JJ as_IN recursive_JJ neural_JJ networks_NNS and_CC convolutional_JJ neural_JJ networks_NNS.
 Our_PRP$ experiments_NNS show_VBP that_IN the_DT Visual_NNP pathway_NN in_IN general_JJ pays_NNS special_JJ attention_NN to_TO syntactic_JJ categories_NNS that_WDT carry_VBP semantic_JJ content_NN, and_CC particularly_RB to_TO nouns_NNS. More_RBR surprisingly_RB, this_DT pathway_NN also_RB learns_VBZ to_TO treat_VB word_NN types_NNS differently_RB depending_VBG on_IN their_PRP$ grammatical_JJ function_NN and_CC their_PRP$ position_NN in_IN the_DT sequential_JJ structure_NN of_IN the_DT sentence_NN. In_IN contrast_NN, the_DT Textual_NNP pathway_NN and_CC the_DT standalone_NN language_NN model_NN are_VBP especially_RB sensitive_JJ to_TO the_DT local_JJ syntactic_JJ characteristics_NNS of_IN the_DT input_NN sentences_NNS. Further_JJ analysis_NN of_IN the_DT most_RBS informative_JJ n-gram_JJ contexts_NN for_IN each_DT model_NN shows_VBZ that_IN whereas_IN the_DT Visual_NNP pathway_NN is_VBZ mostly_RB sensitive_JJ to_TO lexical_JJ (i.e._FW, token_VB n-gram_JJ) contexts_NN, the_DT language_NN models_NNS react_VBP more_JJR strongly_RB to_TO abstract_VB contexts_NN (i.e._FW, dependency_NN relation_NN n-grams_NNS) that_WDT represent_VBP syntactic_JJ constructions_NNS.
 2_CD. Related_JJ Work_NN
 The_DT direct_JJ predecessors_NNS of_IN modern_JJ architectures_NNS were_VBD first_RB proposed_VBN in_IN the_DT seminal_JJ paper_NN by_IN Elman_NNP (1990_CD). He_PRP modifies_VBZ the_DT RNN_NNP architecture_NN of_IN Jordan_NNP (1986_CD) by_IN changing_VBG the_DT output-to-memory_JJ feedback_NN connections_NNS to_TO hidden-to-memory_JJ recurrence_NN, enabling_VBG Elman_NNP networks_NNS to_TO represent_VB arbitrary_JJ dynamic_JJ systems_NNS. Elman_NNP (1991_CD) trains_VBZ an_DT RNN_NNP on_IN a_DT small_JJ synthetic_JJ sentence_NN data_NNS set_NN and_CC analyzes_VBZ the_DT activation_NN patterns_NNS of_IN the_DT hidden_JJ layer_NN. His_PRP$ analysis_NN shows_VBZ that_IN these_DT distributed_VBN representations_NNS encode_VBP lexical_JJ categories_NNS, grammatical_JJ relations_NNS, and_CC hierarchical_JJ constituent_NN structures_NNS. Giles_NNP et_CC al_NN. (1991_CD) train_NN RNNs_NNP similar_JJ to_TO Elman_NNP networks_NNS on_IN strings_NNS generated_VBN by_IN small_JJ deterministic_JJ regular_JJ grammars_NNS with_IN the_DT objective_NN to_TO recognize_VB grammatical_JJ and_CC reject_JJ ungrammatical_JJ strings_NNS, and_CC develop_VB the_DT dynamic_JJ state_NN partitioning_NN technique_NN to_TO extract_VB the_DT learned_JJ grammar_NN from_IN the_DT networks_NNS in_IN the_DT form_NN of_IN deterministic_JJ finite_JJ state_NN automatons_NNS.
 More_RBR closely_RB related_VBN is_VBZ the_DT recent_JJ work_NN of_IN Li_NNP et_FW al_NN. (2016a_CD), who_WP develop_VBP techniques_NNS for_IN a_DT deeper_JJR understanding_NN of_IN the_DT activation_NN patterns_NNS of_IN RNNs_NNP, but_CC focus_VBP on_IN models_NNS with_IN modern_JJ architectures_NNS trained_VBN on_IN large_JJ scale_JJ data_NNS sets_NNS. More_RBR specifically_RB, they_PRP train_VBP long_JJ short-term_JJ memory_NN networks_NNS (LSTMs_NNP) (Hochreiter_NNP and_CC Schmidhuber_NNP, 1997_CD) for_IN phrase-level_JJ sentiment_NN analysis_NN and_CC present_JJ novel_NN methods_NNS to_TO explore_VB the_DT inner_JJ workings_NNS of_IN RNNs_NNP. They_PRP measure_VBP the_DT salience_NN of_IN tokens_NNS in_IN sentences_NNS by_IN taking_VBG the_DT first-order_JJ derivatives_NNS of_IN the_DT loss_NN with_IN respect_NN to_TO the_DT word_NN embeddings_NNS and_CC provide_VB evidence_NN that_IN LSTMs_NNP can_MD learn_VB to_TO attend_VB to_TO important_JJ tokens_NNS in_IN sentences_NNS. Furthermore_RB, they_PRP plot_VBP the_DT activation_NN values_NNS of_IN hidden_JJ units_NNS through_IN time_NN using_VBG heat_NN maps_NNS and_CC visualize_VB local_JJ semantic_JJ compositionality_NN in_IN RNNs_NNP. In_IN comparison_NN, the_DT present_JJ work_NN goes_VBZ beyond_IN the_DT importance_NN of_IN single_JJ words_NNS and_CC focuses_VBZ more_RBR on_IN exploring_VBG structure_NN learning_NN in_IN RNNs_NNP, as_RB well_RB as_IN on_IN developing_VBG methods_NNS for_IN a_DT comparative_JJ analysis_NN between_IN RNNs_NNP that_WDT are_VBP focused_VBN on_IN different_JJ modalities_NNS (language_NN vs._FW vision_NN).
 Adding_VBG an_DT explicit_JJ attention_NN mechanism_NN that_WDT allows_VBZ the_DT RNNs_NNP to_TO focus_VB on_IN different_JJ parts_NNS of_IN the_DT input_NN was_VBD recently_RB introduced_VBN by_IN Bahdanau_NNP, Cho_NNP, and_CC Bengio_NNP (2015_CD) in_IN the_DT context_NN of_IN extending_VBG the_DT sequence-to-sequence_NN RNN_NNP architecture_NN for_IN neural_JJ machine_NN translation_NN. On_IN the_DT decoding_JJ side_NN this_DT neural_JJ module_NN assigns_NNS weights_NNS to_TO the_DT hidden_JJ states_NNS of_IN the_DT decoder_NN, which_WDT allows_VBZ the_DT decoder_NN to_TO selectively_RB pay_VB varying_VBG degrees_NNS of_IN attention_NN to_TO different_JJ phrases_NNS in_IN the_DT source_NN sentence_NN at_IN different_JJ decoding_VBG time-steps_NN. They_PRP also_RB provide_VBP qualitative_JJ analysis_NN by_IN visualizing_VBG the_DT attention_NN weights_NNS and_CC exploring_VBG the_DT importance_NN of_IN the_DT source_NN encodings_NNS at_IN various_JJ decoding_VBG steps_NNS. Similarly_RB Rockt_NNP schel_NN et_NN al_NN. (2016_CD) use_VBP an_DT attentive_JJ neural_JJ network_NN architecture_NN to_TO perform_VB natural_JJ language_NN inference_NN and_CC visualize_NN which_WDT parts_NNS of_IN the_DT hypotheses_NNS and_CC premises_VBZ the_DT model_NN pays_VBZ attention_NN to_TO when_WRB deciding_VBG on_IN the_DT entailment_NN relationship_NN. Conversely_RB, the_DT present_JJ work_NN focuses_NNS on_IN RNNs_NNP without_IN an_DT explicit_JJ attention_NN mechanism_NN.
 Karpathy_NNP, Johnson_NNP, and_CC Li_NNP (2016_CD) also_RB take_VBP up_RP the_DT challenge_NN of_IN rendering_VBG RNN_NNP activation_NN patterns_NNS understandable_JJ, but_CC use_VBP character_JJR level_NN language_NN models_NNS and_CC rather_RB than_IN taking_VBG a_DT linguistic_JJ point_NN of_IN view_NN, focus_NN on_IN error_NN analysis_NN and_CC training_VBG dynamics_NNS of_IN LSTMs_NNP and_CC GRUs_NNP. They_PRP show_VBP that_IN certain_JJ dimensions_NNS in_IN the_DT RNN_NNP hidden_JJ activation_NN vectors_NNS have_VBP specific_JJ and_CC interpretable_JJ functions_NNS. Similarly_RB, Li_NNP et_CC al_NN. (2016b_CD) use_NN a_DT convolutional_JJ neural_JJ network_NN (CNN_NNP) based_VBN on_IN the_DT architecture_NN of_IN Krizhevsky_NNP, Sutskever_NNP, and_CC Hinton_NNP (2012_CD), and_CC train_VB it_PRP on_IN the_DT ImageNet_NNP data_NN set_NN using_VBG different_JJ random_JJ initializations_NNS. For_IN each_DT layer_NN in_IN all_DT networks_NNS they_PRP store_VBP the_DT activation_NN values_NNS produced_VBN on_IN the_DT validation_NN set_NN of_IN the_DT ImageNet_NNP Large_NNP Scale_NNP Visual_NNP Recognition_NNP Competition_NNP and_CC align_VB similar_JJ neurons_NNS of_IN different_JJ networks_NNS. They_PRP conclude_VBP that_IN although_IN some_DT features_NNS are_VBP learned_VBN across_IN networks_NNS, some_DT seem_VBP to_TO depend_VB on_IN the_DT initialization_NN. Other_JJ works_NNS on_IN visualizing_VBG the_DT role_NN of_IN individual_JJ hidden_JJ units_NNS in_IN deep_JJ models_NNS for_IN vision_NN synthesize_NN images_NNS by_IN optimizing_VBG random_NN images_NNS through_IN backpropagation_NN to_TO maximize_VB the_DT activity_NN of_IN units_NNS (Erhan_NNP et_RB al._RB, 2009_CD; Simonyan_NNP, Vedaldi_NNP, and_CC Zisserman_NNP, 2014_CD; Yosinski_NNP et_FW al._NN, 2015_CD; Nguyen_NNP, Yosinski_NNP, and_CC Clune_NNP, 2016_CD) or_CC to_TO approximate_VB the_DT activation_NN vectors_NNS of_IN particular_JJ layers_NNS (Dosovitskiy_NNP and_CC Brox_NNP, 2015_CD; Mahendran_NNP and_CC Vedaldi_NNP, 2016_CD).
 While_IN this_DT paper_NN was_VBD under_IN review_NN, a_DT number_NN of_IN articles_NNS appeared_VBD that_IN also_RB investigate_VB linguistic_JJ representations_NNS in_IN LSTM_NNP architectures_NNS. In_IN an_DT approach_NN similar_JJ to_TO ours_VB, Li_NNP, Monroe_NNP, and_CC Jurafsky_NNP (2016_CD) study_VBD the_DT contribution_NN of_IN individual_JJ input_NN tokens_NNS as_RB well_RB as_IN hidden_VBN units_NNS and_CC word_NN embedding_VBG dimensions_NNS by_IN erasing_VBG them_PRP from_IN the_DT representation_NN and_CC analyzing_VBG how_WRB this_DT affects_VBZ the_DT model_NN. They_PRP focus_VBP on_IN text-only_JJ tasks_NNS and_CC do_VBP not_RB take_VB other_JJ modalities_NNS such_JJ as_IN visual_JJ input_NN into_IN account_NN. Adi_NNP et_CC al_NN. (2017_CD) take_VB an_DT alternative_JJ approach_NN by_IN introducing_VBG prediction_NN tasks_NNS to_TO analyze_VB information_NN encoded_VBN in_IN sentence_NN embeddings_NNS about_IN sentence_NN length_NN, sentence_NN content_NN, and_CC word_NN order_NN. Finally_RB, Linzen_NNP, Dupoux_NNP, and_CC Goldberg_NNP (2016_CD) examine_VB the_DT acquisition_NN of_IN long-distance_NN dependencies_NNS through_IN the_DT study_NN of_IN number_NN agreement_NN in_IN different_JJ variations_NNS of_IN an_DT LSTM_NNP model_NN with_IN different_JJ objectives_NNS (number_NN prediction_NN, grammaticality_NN judgment_NN, and_CC language_NN modeling_NN). Their_PRP$ results_NNS show_VBP that_IN such_JJ dependencies_NNS can_MD be_VB captured_VBN with_IN very_RB high_JJ accuracy_NN when_WRB the_DT model_NN receives_VBZ a_DT strong_JJ supervision_NN signal_NN (i.e._FW, whether_IN the_DT subject_NN is_VBZ plural_JJ or_CC singular_JJ), but_CC simple_JJ language_NN models_NNS still_RB capture_VBP the_DT majority_NN of_IN test_NN cases_NNS. Whereas_IN they_PRP focus_VBP on_IN an_DT in-depth_JJ analysis_NN of_IN a_DT single_JJ phenomenon_NN, in_IN our_PRP$ work_NN we_PRP are_VBP interested_JJ in_IN methods_NNS that_WDT make_VBP it_PRP possible_JJ to_TO uncover_VB a_DT broad_JJ variety_NN of_IN patterns_NNS of_IN behavior_NN in_IN RNNs_NNP.
 In_IN general_JJ, there_EX has_VBZ been_VBN a_DT growing_VBG interest_NN within_IN computer_NN vision_NN in_IN understanding_VBG deep_JJ models_NNS, with_IN a_DT number_NN of_IN papers_NNS dedicated_VBN to_TO visualizing_VBG learned_VBN CNN_NNP filters_NNS and_CC pixel_NN saliencies_NNS (Simonyan_NNP, Vedaldi_NNP, and_CC Zisserman_NNP, 2014_CD; Mahendran_NNP and_CC Vedaldi_NNP, 2015_CD; Yosinski_NNP et_FW al._NN, 2015_CD). These_DT techniques_NNS have_VBP also_RB led_VBN to_TO improvements_NNS in_IN model_NN performance_NN (Eigen_NNP et_RB al._RB, 2014_CD) and_CC transferability_NN of_IN features_NNS (Zhou_NNP et_RB al._RB, 2015_CD). To_TO date_NN there_RB has_VBZ been_VBN much_RB less_RBR work_NN on_IN such_JJ issues_NNS within_IN computational_JJ linguistics_NNS. We_PRP aim_VBP to_TO fill_VB this_DT gap_NN by_IN adapting_VBG existing_VBG methods_NNS as_RB well_RB as_IN developing_VBG novel_JJ techniques_NNS to_TO explore_VB the_DT linguistic_JJ structure_NN learned_VBN by_IN recurrent_NN networks_NNS. </introduction_NN>



 <method_NN> 3_CD. Models_NNS
 In_IN our_PRP$ analyses_NNS of_IN the_DT acquired_JJ linguist_NN knowledge_NN, we_PRP apply_VBP our_PRP$ methods_NNS to_TO the_DT following_JJ models_NNS:
 Imaginet_NN: A_DT multi-modal_JJ GRU_NNP network_NN consisting_VBG of_IN two_CD pathways_NNS, Visual_NNP and_CC Textual_NNP, coupled_VBD via_IN word_NN embeddings_NNS.
 LM_NN: A_DT (unimodal_JJ) language_NN model_NN consisting_VBG of_IN a_DT GRU_NNP network_NN.
 Sum_NN: A_DT network_NN with_IN the_DT same_JJ objective_NN as_IN the_DT Visual_NNP pathway_NN of_IN Imaginet_NNP, but_CC that_IN uses_VBZ sum_NN of_IN word_NN embeddings_NNS instead_RB of_IN a_DT GRU_NNP.
 The_DT rest_NN of_IN this_DT section_NN gives_VBZ a_DT detailed_JJ description_NN of_IN these_DT models_NNS.
 3.1_CD Gated_NNP Recurrent_NNP Neural_NNP Networks_NNP
 One_CD of_IN the_DT main_JJ difficulties_NNS for_IN training_VBG traditional_JJ Elman_NNP networks_NNS arises_VBZ from_IN the_DT fact_NN that_IN they_PRP overwrite_VBP their_PRP$ hidden_JJ states_NNS at_IN every_DT time_NN step_NN with_IN a_DT new_JJ value_NN computed_VBN from_IN the_DT current_JJ input_NN xt_NN and_CC the_DT previous_JJ hidden_NN state_NN ht_VBD 1_CD. Similarly_RB to_TO LSTMs_NNP, GRU_NNP networks_VBZ introduce_VB a_DT mechanism_NN that_WDT facilitates_VBZ the_DT retention_NN of_IN information_NN over_IN multiple_JJ time_NN steps_NNS. Specifically_RB, the_DT GRU_NNP computes_VBZ the_DT hidden_JJ state_NN at_IN current_JJ time_NN step_NN ht_RB, as_IN the_DT linear_JJ combination_NN of_IN previous_JJ activation_NN ht_NN 1_CD, and_CC a_DT new_JJ candidate_NN activation_NN h_NN th~t_NN:
 GRU_NNP (ht_JJ 1_CD, xt_NN) =_NN (1_CD zt_NN) ¡Ñht_VBZ 1+zt¡Ñh_CD tGRU_NN (ht_JJ 1_CD, xt_NN) =_NN (1_CD zt_NN) ¡Ñht_VBZ 1+zt¡Ñh~t_CD (1_CD)
 where_WRB ¡Ñ_NN is_VBZ elementwise_JJ multiplication_NN, and_CC the_DT update_JJ gate_NN activation_NN zt_NN determines_VBZ the_DT amount_NN of_IN new_JJ information_NN mixed_VBN in_IN the_DT current_JJ state_NN:
 zt=¦Òs_NN (Wzxt+Uzht_NNP 1_CD) zt=¦Òs_NN (Wzxt+Uzht_NNP 1_CD) (2_CD)
 The_DT candidate_NN activation_NN is_VBZ computed_VBN as_IN:
 h_NN t=¦Ò_NN (Wxt+U_NNP (rt¡Ñht_VB 1_CD)) h~t=¦Ò_NN (Wxt+U_NNP (rt¡Ñht_VB 1_CD)) (3_CD)
 The_DT reset_NN gate_NN rt_NN determines_VBZ how_WRB much_JJ of_IN the_DT current_JJ input_NN xt_NN is_VBZ mixed_VBN in_IN the_DT previous_JJ state_NN ht_VBD 1_CD to_TO form_VB the_DT candidate_NN activation_NN:
 rt=¦Òs_NN (Wrxt+Urht_NNP 1_CD) rt=¦Òs_NN (Wrxt+Urht_NNP 1_CD) (4_CD)
 where_WRB W_NNP, U_NNP, Wz_NNP, Uz_NNP, Wr_NNP and_CC Ur_NNP are_VBP learnable_JJ parameters_NNS.
 3.2_CD Imaginet_NN
 Imaginet_NNP, introduced_VBD in_IN Chrupa_NNP a_DT, K¨¢d¨¢r_NNP, and_CC Alishahi_NNP (2015_CD), is_VBZ a_DT multi-modal_JJ GRU_NNP network_NN architecture_NN that_WDT learns_VBZ visually_RB grounded_VBN meaning_NN representations_NNS from_IN textual_JJ and_CC visual_JJ input_NN. It_PRP acquires_VBZ linguistic_JJ knowledge_NN through_IN language_NN comprehension_NN, by_IN receiving_VBG a_DT description_NN of_IN a_DT scene_NN and_CC trying_VBG to_TO visualize_VB it_PRP through_IN predicting_VBG a_DT visual_JJ representation_NN for_IN the_DT textual_JJ description_NN, while_IN concurrently_RB predicting_VBG the_DT next_JJ word_NN in_IN the_DT sequence_NN.
 Figure_NN 1_CD shows_VBZ the_DT structure_NN of_IN Imaginet_NNP. As_IN can_MD be_VB seen_VBN from_IN the_DT figure_NN, the_DT model_NN consists_VBZ of_IN two_CD GRU_NNP pathways_NNS, Textual_NNP and_CC Visual_NNP, with_IN a_DT shared_VBN word_NN embedding_VBG matrix_NN. The_DT inputs_NNS to_TO the_DT model_NN are_VBP pairs_NNS of_IN image_NN descriptions_NNS and_CC their_PRP$ corresponding_NN images_NNS. The_DT Textual_NNP pathway_NN predicts_VBZ the_DT next_JJ word_NN at_IN each_DT position_NN in_IN the_DT sequence_NN of_IN words_NNS in_IN each_DT caption_NN, whereas_IN the_DT Visual_NNP pathway_NN predicts_VBZ a_DT visual_JJ representation_NN of_IN the_DT image_NN that_WDT depicts_VBZ the_DT scene_NN described_VBN by_IN the_DT caption_NN after_IN the_DT final_JJ word_NN is_VBZ received_VBN.
 Formally_RB, each_DT sentence_NN is_VBZ mapped_VBN to_TO two_CD sequences_NNS of_IN hidden_JJ states_NNS, one_CD by_IN Visual_NNP and_CC the_DT other_JJ by_IN Textual_NNP.
 At_IN each_DT time_NN step_NN Textual_NNP predicts_VBZ the_DT next_JJ word_NN in_IN the_DT sentence_NN S_NNP from_IN its_PRP$ current_JJ hidden_JJ state_NN hTthtT_NN, and_CC Visual_NNP predicts_VBZ the_DT image-vector1i_JJ i^_NN from_IN its_PRP$ last_JJ hidden_JJ representation_NN hVthtV_NN.
 The_DT loss_NN function_NN is_VBZ a_DT multi-task_JJ objective_NN that_WDT penalizes_VBZ error_NN on_IN the_DT visual_JJ and_CC the_DT textual_JJ targets_NNS simultaneously_RB. The_DT objective_JJ combines_NNS cross-entropy_JJ loss_NN LT_NNP for_IN the_DT word_NN predictions_NNS and_CC cosine_NN distance_NN LV_NNP for_IN the_DT image_NN predictions,2_NN weighting_VBG them_PRP with_IN the_DT parameter_NN ¦Á_NNP (set_VBN to_TO 0.1_CD).

 For_IN more_JJR details_NNS about_IN the_DT Imaginet_NNP model_NN and_CC its_PRP$ performance_NN, see_VBP Chrupa_NNP a_DT, K¨¢d¨¢r_NNP, and_CC Alishahi_NNP (2015_CD). Note_NN that_IN we_PRP introduce_VBP a_DT small_JJ change_NN in_IN the_DT image_NN representation_NN: We_PRP observe_VBP that_IN using_VBG standardized_JJ image_NN vectors_NNS, where_WRB each_DT dimension_NN is_VBZ transformed_VBN by_IN subtracting_VBG the_DT mean_NN and_CC dividing_NN by_IN standard_JJ deviation_NN, improves_VBZ performance_NN.
 3.3_CD Unimodal_JJ Language_NNP Model_NNP
 The_DT model_NN LM_NNP is_VBZ a_DT language_NN model_NN analogous_JJ to_TO the_DT Textual_NNP pathway_NN of_IN Imaginet_NNP with_IN the_DT difference_NN that_IN its_PRP$ word_NN embeddings_NNS are_VBP not_RB shared_VBN, and_CC its_PRP$ loss_NN function_NN is_VBZ the_DT cross-entropy_NN on_IN word_NN prediction_NN. Using_VBG this_DT model_NN we_PRP remove_VBP the_DT visual_JJ objective_NN as_IN a_DT factor_NN, as_IN the_DT model_NN does_VBZ not_RB use_VB the_DT images_NNS corresponding_VBG to_TO captions_NNS in_IN any_DT way_NN.
 3.4_CD Sum_NNP of_IN Word_NNP Embeddings_NNP
 The_DT model_NN Sum_NNP is_VBZ a_DT stripped-down_JJ version_NN of_IN the_DT Visual_NNP pathway_NN, which_WDT does_VBZ not_RB share_NN word_NN embeddings_NNS, only_RB uses_VBZ the_DT cosine_JJ loss_NN function_NN, and_CC replaces_VBZ the_DT GRU_NNP network_NN with_IN a_DT summation_NN over_IN word_NN embeddings_NNS. This_DT removes_VBZ the_DT effect_NN of_IN word_NN order_NN from_IN consideration_NN. We_PRP use_VBP this_DT model_NN as_IN a_DT baseline_NN in_IN the_DT sections_NNS that_WDT focus_VBP on_IN language_NN structure_NN. </method_NNP>




 <discussion_NN> 4_CD. Experiments_NNS
 In_IN this_DT section_NN, we_PRP report_VBP a_DT series_NN of_IN experiments_NNS in_IN which_WDT we_PRP explore_VBP the_DT kinds_NNS of_IN linguistic_JJ regularities_NNS the_DT networks_NNS learn_VBP from_IN word-level_JJ input_NN. In_IN Section_NNP 4.1_CD we_PRP introduce_VBP omission_JJ score_NN, a_DT metric_JJ to_TO measure_VB the_DT contribution_NN of_IN each_DT token_NN to_TO the_DT prediction_NN of_IN the_DT networks_NNS, and_CC in_IN Section_NNP 4.2_CD we_PRP analyze_VBP how_WRB omission_NN scores_NNS are_VBP distributed_VBN over_IN dependency_NN relations_NNS and_CC part-of-speech_JJ categories_NNS. In_IN Section_NNP 4.3_CD we_PRP investigate_VBP the_DT extent_NN to_TO which_WDT the_DT importance_NN of_IN words_NNS for_IN the_DT different_JJ networks_NNS depends_VBZ on_IN the_DT words_NNS themselves_PRP, their_PRP$ sequential_JJ position_NN, and_CC their_PRP$ grammatical_JJ function_NN in_IN the_DT sentences_NNS. Finally_RB, in_IN Section_NNP 4.4_CD we_PRP systematically_RB compare_VBP the_DT types_NNS of_IN n-gram_JJ contexts_NN that_IN trigger_JJR individual_JJ dimensions_NNS in_IN the_DT hidden_JJ layers_NNS of_IN the_DT networks_NNS, and_CC discuss_VB their_PRP$ level_NN of_IN abstractness_NN.
 In_IN all_PDT these_DT experiments_NNS we_PRP report_VBP our_PRP$ findings_NNS based_VBN on_IN the_DT Imaginet_NNP model_NN, and_CC whenever_WRB appropriate_JJ compare_NN it_PRP with_IN our_PRP$ two_CD other_JJ models_NNS LM_NNP and_CC Sum_NNP. For_IN all_PDT the_DT experiments_NNS, we_PRP trained_VBD the_DT models_NNS on_IN the_DT training_NN portion_NN of_IN the_DT MSCOCO_NNP image-caption_NN data_NNS set_NN (Lin_NNP et_RB al._RB, 2014_CD), and_CC analyzed_VBD the_DT representations_NNS of_IN the_DT sentences_NNS in_IN the_DT validation_NN set_VBN corresponding_VBG to_TO 5000_CD randomly_RB chosen_VBN images_NNS. The_DT target_NN image_NN representations_NNS were_VBD extracted_VBN from_IN the_DT pre-softmax_JJ layer_NN of_IN the_DT 16-layer_JJ CNN_NNP of_IN Simonyan_NNP and_CC Zisserman_NNP (2015_CD).
 4.1_CD Computing_VBG Omission_NN Scores_NNS
 We_PRP propose_VBP a_DT novel_NN technique_NN for_IN interpreting_VBG the_DT activation_NN patterns_NNS of_IN neural_JJ networks_NNS trained_VBN on_IN language_NN tasks_NNS from_IN a_DT linguistic_JJ point_NN of_IN view_NN, and_CC focus_VB on_IN the_DT high-level_JJ understanding_NN of_IN what_WP parts_NNS of_IN the_DT input_NN sentence_NN the_DT networks_NNS pay_VBP most_RBS attention_NN to_TO. Furthermore_RB, we_PRP investigate_VBP whether_IN the_DT networks_NNS learn_VBP to_TO assign_VB different_JJ amounts_NNS of_IN importance_NN to_TO tokens_NNS, depending_VBG on_IN their_PRP$ position_NN and_CC grammatical_JJ function_NN in_IN the_DT sentences_NNS.
 In_IN all_PDT the_DT models_NNS the_DT full_JJ sentences_NNS are_VBP represented_VBN by_IN the_DT activation_NN vector_NN at_IN the_DT end-of-sentence_JJ symbol_NN (hend_NN). We_PRP measure_VBP the_DT salience_NN of_IN each_DT word_NN Si_NNP in_IN an_DT input_NN sentence_NN S1_NNP: nbased_VBN on_IN how_WRB much_JJ the_DT representation_NN of_IN the_DT partial_JJ sentence_NN S_NNP i_NN =_VBP S1_NN: i_NN 1Si_CD +1_NN: n_NN, with_IN the_DT omitted_VBN word_NN Si_NNP, deviates_VBZ from_IN that_DT of_IN the_DT original_JJ sentence_NN representation_NN. For_IN example_NN, the_DT distance_NN between_IN hend_NN (theblackdogisrunning_VBG) and_CC hend_NN (thedogisrunning_VBG) determines_VBZ the_DT importance_NN of_IN black_JJ in_IN the_DT first_JJ sentence_NN. We_PRP introduce_VBP the_DT measure_NN omission_NN (i_JJ, S_NNP) for_IN estimating_VBG the_DT salience_NN of_IN a_DT word_NN Si_NNP.
 Figure_NN 2_CD demonstrates_VBZ the_DT omission_NN scores_VBZ for_IN the_DT LM_NNP, Visual_NNP, and_CC Textual_NNP models_NNS for_IN an_DT example_NN caption_NN. Figure_NN 3_CD shows_VBZ the_DT images_NNS retrieved_VBN by_IN Visual_NNP for_IN the_DT full_JJ caption_NN and_CC for_IN the_DT one_NN with_IN the_DT word_NN baby_NN omitted_VBD. The_DT images_NNS are_VBP retrieved_VBN from_IN the_DT validation_NN set_NN of_IN MSCOCO_NNP by_IN: 1_CD) computing_VBG the_DT image_NN representation_NN of_IN the_DT given_VBN sentence_NN with_IN Visual_NNP; 2_CD) extracting_VBG the_DT CNN_NNP features_NNS for_IN the_DT images_NNS from_IN the_DT set_NN; and_CC 3_CD) finding_VBG the_DT image_NN that_WDT minimizes_VBZ the_DT cosine_NN distance_NN to_TO the_DT query_NN. The_DT omission_NN scores_VBZ for_IN Visual_NNP show_NN that_IN the_DT model_NN paid_VBD attention_NN mostly_RB to_TO baby_VB and_CC bed_VB and_CC slightly_RB to_TO laptop_VB, and_CC retrieved_VBD an_DT image_NN depicting_VBG a_DT baby_NN sitting_VBG on_IN a_DT bed_NN with_IN a_DT laptop_NN. Removing_VBG the_DT word_NN baby_NN leads_VBZ to_TO an_DT image_NN that_WDT depicts_VBZ an_DT adult_NN male_NN lying_VBG on_IN a_DT bed_NN. Figure_NN 2_CD also_RB shows_VBZ that_IN in_IN contrast_NN to_TO Visual_NNP, Textual_NNP distributes_VBZ its_PRP$ attention_NN more_RBR evenly_RB across_IN time_NN steps_NNS instead_RB of_IN focusing_VBG on_IN the_DT types_NNS of_IN words_NNS related_VBN to_TO the_DT corresponding_JJ visual_JJ scene_NN. The_DT peaks_NNS for_IN LM_NNP are_VBP the_DT same_JJ as_IN for_IN Textual_NNP, but_CC the_DT variance_NN of_IN the_DT omission_NN scores_VBZ is_VBZ higher_JJR, suggesting_VBG that_IN the_DT unimodal_JJ language_NN model_NN is_VBZ more_RBR sensitive_JJ overall_JJ to_TO input_VB perturbations_NNS than_IN Textual_NNP.
 Omission_NN scores_NNS for_IN the_DT example_NN sentence_NN a_DT baby_NN sits_VBZ on_IN a_DT bed_NN laughing_VBG with_IN a_DT laptop_JJ computer_NN openfor_MD LM_NNP and_CC the_DT two_CD pathways_NNS, Textual_NNP and_CC Visual_NNP, of_IN Imaginet_NNP.
 Images_NNS retrieved_VBN for_IN the_DT example_NN sentence_NN a_DT baby_NN sits_VBZ on_IN a_DT bed_NN laughing_VBG with_IN a_DT laptop_JJ computer_NN open_JJ (left_VBN) and_CC the_DT same_JJ sentence_NN with_IN the_DT second_JJ word_NN omitted_VBN (right_JJ).
 4.2_CD Omission_NNP Score_NNP Distributions_NNS
 The_DT omission_NN scores_NNS can_MD be_VB used_VBN not_RB only_RB to_TO estimate_VB the_DT importance_NN of_IN individual_JJ words_NNS, but_CC also_RB of_IN syntactic_JJ categories_NNS. We_PRP estimate_VBP the_DT salience_NN of_IN each_DT syntactic_JJ category_NN by_IN accumulating_VBG the_DT omission_NN scores_NNS for_IN all_DT words_NNS in_IN that_DT category_NN. We_PRP tag_VBP every_DT word_NN in_IN a_DT sentence_NN with_IN the_DT part-of-speech_NN (POS_NNP) category_NN and_CC the_DT dependency_NN relation_NN label_NN of_IN its_PRP$ incoming_NN arc_NN. For_IN example_NN, for_IN the_DT sentence_NN the_DT black_JJ dog_NN, we_PRP get_VBP (the_DT, DT_NNP, det_NN), (black_JJ, JJ_NNP, amod_NN), (dog_NN, NN_NNP, root_NN). Both_DT POS_NNP tagging_NN and_CC dependency_NN parsing_NN are_VBP performed_VBN using_VBG theen_core_web_md_JJ dependency_NN parser_NN from_IN the_DT Spacy_NNP package.3_NN
 Figure_NN 4_CD shows_VBZ the_DT distribution_NN of_IN omission_NN scores_NNS per_IN POS_NNP and_CC dependency_NN label_NN for_IN the_DT two_CD pathways_NNS of_IN Imaginet_NNP and_CC for_IN LM.4_NNP The_DT general_JJ trend_NN is_VBZ that_IN for_IN the_DT Visual_NNP pathway_NN, the_DT omission_NN scores_NNS are_VBP high_JJ for_IN a_DT small_JJ subset_NN of_IN labels¡ªcorresponding_VBG mostly_RB to_TO nouns_VB, less_JJR so_RB for_IN adjectives_NNS and_CC even_RB less_JJR for_IN verbs¡ªand_NN low_NN for_IN the_DT rest_NN (mostly_RB function_NN words_NNS and_CC various_JJ types_NNS of_IN verbs_NN). For_IN Textual_NNP the_DT differences_NNS are_VBP smaller_JJR, and_CC the_DT pathway_NN seems_VBZ to_TO be_VB sensitive_JJ to_TO the_DT omission_NN of_IN most_JJS types_NNS of_IN words_NNS. For_IN LM_NNP the_DT distribution_NN over_IN categories_NNS is_VBZ also_RB relatively_RB uniform_JJ, but_CC the_DT omission_NN scores_NNS are_VBP higher_JJR overall_JJ than_IN for_IN Textual_NNP.
 Distribution_NN of_IN omission_NN scores_NNS for_IN POS_NNP (left_VBN) and_CC dependency_NN labels_NNS (right_NN), for_IN the_DT Textual_NNP and_CC Visual_NNP pathways_NNS and_CC for_IN LM_NNP. Only_RB labels_VBZ that_IN occur_NN at_IN least_JJS 1,250_CD times_NNS are_VBP included_VBN.
 Figure_NN 5_CD compares_VBZ the_DT two_CD pathways_NNS of_IN Imaginet_NNP directly_RB using_VBG the_DT log_NN of_IN the_DT ratio_NN of_IN the_DT Visual_NNP to_TO Textual_NNP omission_NN scores_NNS, and_CC plots_VBZ the_DT distribution_NN of_IN this_DT ratio_NN for_IN different_JJ POS_NNP and_CC dependency_NN labels_NNS. Log_NNP ratios_NNS above_IN zero_NN indicate_VBP stronger_JJR association_NN with_IN the_DT Visualpathway_NNP and_CC below_IN zero_NN with_IN the_DT Textual_NNP pathway_NN. We_PRP see_VBP that_IN in_IN relative_JJ terms_NNS, Visual_NNP is_VBZ more_RBR sensitive_JJ to_TO adjectives_NNS (JJ_NNP), nouns_FW (NNS_NNP, NN_NNP), numerals_NNS (CD_NN), and_CC participles_NNS (VBN_NNP), and_CC Textual_NNP is_VBZ more_RBR sensitive_JJ to_TO determiners_NNS (DT_NNP), pronouns_FW (PRP_NNP), prepositions_NNS (IN_NNP), and_CC finite_JJ verbs_NN (VBZ_NNP, VBP_NNP).
 Distributions_NNS of_IN log_JJ ratios_NNS of_IN omission_NN scores_NNS of_IN Textual_NNP to_TO Visual_NNP per_IN POS_NNP (left_VBN) and_CC dependency_NN labels_NNS (right_NN). Only_RB labels_VBZ that_IN occur_NN at_IN least_JJS 1,250_CD times_NNS are_VBP included_VBN.
 This_DT picture_NN is_VBZ complemented_VBN by_IN the_DT analysis_NN of_IN the_DT relative_JJ importance_NN of_IN dependency_NN relations_NNS: Visual_NNP pays_VBZ most_JJS attention_NN to_TO the_DT relations_NNS amod_VBP, nsubj_RB, root_NN, compound_NN, dobj_NN, and_CC nummod_RB, whereas_JJ Textual_NNP is_VBZ more_RBR sensitive_JJ to_TO det_VB, prep_NN, aux_NN, cc_NN, poss_NN, advmod_NN, prt_NN, and_CC relcl_NN. As_IN expected_VBN, Visual_NNP is_VBZ more_RBR focused_JJ on_IN grammatical_JJ functions_NNS typically_RB filled_VBN by_IN semantically_RB contentful_JJ words_NNS, whereas_NNS Textual_NNP distributes_VBZ its_PRP$ attention_NN more_RBR uniformly_JJ and_CC attends_VBZ relatively_RB more_RBR to_TO purely_RB grammatical_JJ functions_NNS.
 It_PRP is_VBZ worth_JJ noting_VBG, however_RB, the_DT relatively_RB low_JJ omission_NN scores_NNS for_IN verbs_NN in_IN the_DT case_NN of_IN Visual_NNP. One_CD might_MD expect_VB that_IN the_DT task_NN of_IN image_NN prediction_NN from_IN descriptions_NNS requires_VBZ general_JJ language_NN understanding_NN and_CC thus_RB high_JJ omission_NN scores_NNS for_IN all_DT content_JJ words_NNS in_IN general_JJ; however_RB, the_DT results_NNS suggest_VBP that_IN this_DT setting_NN is_VBZ not_RB optimal_JJ for_IN learning_VBG useful_JJ representations_NNS of_IN verbs_NN, which_WDT possibly_RB leads_VBZ to_TO representations_NNS that_WDT are_VBP too_RB task-specific_JJ and_CC not_RB transferable_JJ across_IN tasks_NNS.
 Figure_NN 6_CD shows_VBZ a_DT similar_JJ analysis_NN contrasting_VBG LM_NNP with_IN the_DT Textual_NNP pathway_NN of_IN Imaginet_NNP. The_DT first_JJ observation_NN is_VBZ that_IN the_DT range_NN of_IN values_NNS of_IN the_DT log_NN ratios_NNS is_VBZ narrow_JJ, indicating_VBG that_IN the_DT differences_NNS between_IN these_DT two_CD networks_NNS regarding_VBG which_WDT grammatical_JJ categories_NNS they_PRP are_VBP sensitive_JJ to_TO is_VBZ less_RBR pronounced_JJ than_IN when_WRB comparing_VBG Visual_NNP with_IN Textual_NNP. Although_IN the_DT size_NN of_IN the_DT effect_NN is_VBZ weak_JJ, there_EX also_RB seems_VBZ to_TO be_VB a_DT tendency_NN for_IN the_DT Textual_NNP model_NN to_TO pay_VB relatively_RB more_JJR attention_NN to_TO content_NN and_CC less_JJR to_TO function_VB words_NNS compared_VBN with_IN LM_NNP: It_PRP may_MD be_VB that_IN the_DT Visual_NNP pathway_NN pulls_NNS Textual_NNP in_IN this_DT direction_NN by_IN sharing_VBG word_NN embeddings_NNS with_IN it_PRP.
 Distributions_NNS of_IN log_JJ ratios_NNS of_IN omission_NN scores_NNS of_IN LM_NNP to_TO Textual_NNP per_IN POS_NNP (left_VBN) and_CC dependency_NN labels_NNS (right_NN). Only_RB labels_VBZ that_IN occur_NN at_IN least_JJS 1,250_CD times_NNS are_VBP included_VBN.
 Most_JJS of_IN our_PRP$ findings_NNS up_RB to_TO this_DT point_NN conform_NN reasonably_RB well_RB to_TO prior_VB expectations_NNS about_IN effects_NNS that_WDT particular_VBP learning_VBG objectives_NNS should_MD have_VB. This_DT fact_NN serves_VBZ to_TO validate_VB our_PRP$ methods_NNS. In_IN the_DT next_JJ section_NN we_PRP go_VBP on_IN to_TO investigate_VB less_JJR straightforward_JJ patterns_NNS.
 4.3_CD Beyond_NNP Lexical_NNP Cues_NNP
 Models_NNS that_WDT utilize_VBP the_DT sequential_JJ structure_NN of_IN language_NN have_VBP the_DT capacity_NN to_TO interpret_VB the_DT same_JJ word_NN type_NN differently_RB depending_VBG on_IN the_DT context_NN. The_DT omission_NN score_NN distributions_NNS in_IN Section_NN 4.2_CD show_NN that_IN in_IN the_DT case_NN of_IN Imaginet_NNP the_DT pathways_NNS are_VBP differentially_RB sensitive_JJ to_TO content_VB vs._FW function_NN words_NNS. In_IN principle_NN, this_DT may_MD be_VB either_RB just_RB due_JJ to_TO purely_RB lexical_JJ features_NNS or_CC the_DT model_NN may_MD actually_RB learn_VB to_TO pay_VB more_JJR attention_NN to_TO the_DT same_JJ word_NN type_NN in_IN appropriate_JJ contexts_NN. This_DT section_NN investigates_VBZ to_TO what_WP extent_VB our_PRP$ models_NNS discriminate_VBP between_IN occurrences_NNS of_IN a_DT given_VBN word_NN in_IN different_JJ positions_NNS and_CC grammatical_JJ functions_NNS.
 We_PRP fit_VBP four_CD L2-penalized_JJ linear_JJ regression_NN models_NNS that_WDT predict_VBP the_DT omission_NN scores_VBZ per_IN token_NN with_IN the_DT following_JJ predictor_NN variables_NNS.
 We_PRP use_VBP the_DT 5,000-image_JJ portion_NN of_IN MSCOCO_NNP validation_NN data_NNS for_IN training_NN and_CC test_NN. The_DT captions_NNS contain_VBP about_IN 260,000_CD words_NNS in_IN total_JJ, of_IN which_WDT we_PRP use_VBP 100,000_CD to_TO fit_VB the_DT regression_NN models_NNS. We_PRP then_RB use_VBP the_DT rest_NN of_IN the_DT words_NNS to_TO compute_VB the_DT proportion_NN of_IN variance_NN explained_VBN by_IN the_DT models_NNS. For_IN comparison_NN we_PRP also_RB use_VBP the_DT Sum_NNP model_NN, which_WDT composes_VBZ word_NN embeddings_NNS via_IN summation_NN, and_CC uses_VBZ the_DT same_JJ loss_NN function_NN as_IN Visual_NNP. This_DT model_NN is_VBZ unable_JJ to_TO encode_VB information_NN about_IN word_NN order_NN, and_CC thus_RB is_VBZ a_DT good_JJ baseline_NN here_RB as_IN we_PRP investigate_VBP the_DT sensitivity_NN of_IN the_DT networks_NNS to_TO positional_JJ and_CC structural_JJ cues_NNS.
 Table_JJ 1_CD shows_VBZ the_DT proportion_NN of_IN variance_NN R2_NNP in_IN omission_NN scores_NNS explained_VBN by_IN the_DT linear_JJ regression_NN with_IN the_DT different_JJ predictors_NNS. The_DT raw_JJ R2_NNP scores_VBZ show_VBP that_IN for_IN the_DT language_NN models_NNS LM_NNP and_CC Textual_NNP, the_DT word_NN type_NN predicts_VBZ the_DT omission-score_NN to_TO a_DT much_RB smaller_JJR degree_NN than_IN Visual_NNP. Moreover_RB, adding_VBG information_NN about_IN either_CC the_DT position_NN or_CC the_DT dependency_NN labels_VBZ increases_VBZ the_DT explained_JJ variance_NN for_IN all_DT models_NNS. However_RB, for_IN the_DT Textual_NNP and_CC LM_NNP models_NNS the_DT position_NN of_IN the_DT word_NN adds_VBZ considerable_JJ amount_NN of_IN information_NN. This_DT is_VBZ not_RB surprising_JJ considering_VBG that_IN the_DT omission_NN scores_NNS are_VBP measured_VBN with_IN respect_NN to_TO the_DT final_JJ activation_NN state_NN, and_CC given_VBN the_DT fact_NN that_IN in_IN a_DT language_NN model_NN the_DT recent_JJ history_NN is_VBZ most_RBS important_JJ for_IN accurate_JJ prediction_NN.
 Figure_NN 7_CD offers_VBZ a_DT different_JJ view_NN of_IN the_DT data_NNS, showing_VBG the_DT increase_NN or_CC decrease_NN in_IN R2_NNP for_IN the_DT models_NNS relative_VBP to_TO LR_NNP +pos_NNP to_TO emphasize_VB the_DT importance_NN of_IN syntactic_JJ structure_NN beyond_IN the_DT position_NN in_IN the_DT sentence_NN. Interestingly_RB, for_IN the_DT Visual_NNP model_NN, dependency_NN labels_NNS are_VBP more_RBR informative_JJ than_IN linear_JJ position_NN, hinting_VBG at_IN the_DT importance_NN of_IN syntactic_JJ structure_NN beyond_IN linear_JJ order_NN. There_EX is_VBZ a_DT sizeable_JJ increase_NN in_IN R2_NNP between_IN LR_NNP +pos_NNP and_CC LR_NNP full_JJ in_IN the_DT case_NN of_IN Visual_NNP, suggesting_VBG that_IN the_DT omission_NN scores_VBZ for_IN Visual_NNP depend_NN on_IN the_DT words_NNS '_POS grammatical_JJ function_NN in_IN sentences_NNS, even_RB after_IN controlling_VBG for_IN word_NN identity_NN and_CC linear_JJ position_NN. In_IN contrast_NN, adding_VBG additional_JJ information_NN on_IN top_NN of_IN lexical_JJ features_NNS in_IN the_DT case_NN of_IN Sumincreases_NNP the_DT explained_VBN variance_NN only_RB slightly_RB, which_WDT is_VBZ most_RBS likely_JJ due_JJ to_TO the_DT unseen_JJ words_NNS in_IN the_DT held_NN out_RP set_NN.
 Proportion_NN of_IN variance_NN in_IN omission_NN scores_NNS explained_VBN by_IN the_DT linear_JJ regression_NN models_NNS for_IN Sum_NNP, LM_NNP, Visual_NNP, and_CC Textual_NNP relative_VBP to_TO regressing_VBG on_IN word_NN identity_NN and_CC position_NN only_RB.
 Overall_JJ, when_WRB regressing_VBG on_IN word_NN identities_NNS, word_NN position_NN, and_CC dependency_NN labels_NNS, the_DT Visual_NNP model_NN 's_POS omission_NN scores_NNS are_VBP the_DT hardest_JJS to_TO predict_NN of_IN the_DT four_CD models_NNS. This_DT suggests_VBZ that_IN Visual_NNP may_MD be_VB encoding_VBG additional_JJ structural_JJ features_NNS not_RB captured_VBN by_IN these_DT predictors_NNS. We_PRP will_MD look_VB more_JJR deeply_RB into_IN such_JJ potential_JJ features_NNS in_IN the_DT following_JJ sections_NNS.
 4.3.1_CD Sensitivity_NNP to_TO Grammatical_NNP Function_NNP
 In_IN order_NN to_TO find_VB out_RP some_DT of_IN the_DT specific_JJ syntactic_JJ configurations_NNS leading_VBG to_TO an_DT increase_NN in_IN R2_NNP between_IN the_DT LR_NNP word_NN and_CC LR_NNP +dep_NNP predictors_NNS in_IN the_DT case_NN of_IN Visual_NNP, we_PRP next_JJ considered_VBN all_DT word_NN types_VBZ with_IN occurrence_NN counts_NNS of_IN at_IN least_JJS 100_CD and_CC ranked_VBD them_PRP according_VBG to_TO how_WRB much_JJ better_JJR, on_IN average_NN, LR_NNP +dep_NNP predicted_VBD their_PRP$ omission_NN scores_NNS compared_VBN with_IN LR_NNP word_NN.
 Figure_NN 8_CD shows_VBZ the_DT per-dependency_NN omission_NN score_NN distributions_NNS for_IN seven_CD top-ranked_JJ words_NNS. There_EX are_VBP clear_JJ and_CC large_JJ differences_NNS in_IN how_WRB these_DT words_NNS impact_VBP the_DT network_NN 's_POS representation_NN, depending_VBG on_IN what_WP grammatical_JJ function_NN they_PRP fulfill_VBP. They_PRP all_DT have_VBP large_JJ omission_NN scores_NNS when_WRB they_PRP occur_VBP as_IN nsubj_NN (nominal_JJ subject_NN) or_CC root_NN, likely_RB because_IN these_DT grammatical_JJ functions_NNS typically_RB have_VBP a_DT large_JJ contribution_NN to_TO the_DT complete_JJ meaning_NN of_IN a_DT sentence_NN. Conversely_RB, all_DT have_VBP small_JJ omission_NN scores_NNS when_WRB appearing_VBG as_IN conj_NN (conjunct_JJ): this_DT is_VBZ probably_RB because_IN in_IN this_DT position_NN they_PRP share_NN their_PRP$ contribution_NN with_IN the_DT first_JJ, often_RB more_RBR important_JJ, member_NN of_IN the_DT conjunction¡ªfor_JJ example_NN, in_IN A_NNP cow_NN and_CC its_PRP$ baby_NN eating_VBG grass_NN.
 4.3.2_CD Sensitivity_NNP to_TO Linear_VB Structure_NN
 As_IN observed_VBN in_IN Section_NNP 4.3_CD, adding_VBG extra_JJ information_NN about_IN the_DT position_NN of_IN words_NNS explains_VBZ more_JJR of_IN the_DT variance_NN in_IN the_DT case_NN of_IN Visual_NNP and_CC especially_RB Textual_NNP and_CC LM_NNP. Figure_NN 9_CD shows_VBZ the_DT coefficients_NNS corresponding_VBG to_TO the_DT position_NN variables_NNS in_IN LR_NNP full_JJ. Because_IN the_DT omission_NN scores_NNS are_VBP measured_VBN at_IN the_DT end-of-sentence_NN token_NN, the_DT expectation_NN is_VBZ that_IN for_IN Textual_NNP and_CC LM_NNP, as_IN language_NN models_NNS, the_DT words_NNS appearing_VBG closer_NN to_TO the_DT end_NN of_IN the_DT sentence_NN would_MD have_VB a_DT stronger_JJR effect_NN on_IN the_DT omission_NN scores_NNS. This_DT seems_VBZ to_TO be_VB confirmed_VBN by_IN the_DT plot_NN as_IN the_DT coefficients_NNS for_IN these_DT two_CD networks_NNS up_RB until_IN the_DT antepenult_NN are_VBP all_DT negative_JJ.
 For_IN the_DT Visual_NNP model_NN it_PRP is_VBZ less_RBR clear_JJ what_WP to_TO expect_VB: On_IN the_DT one_CD hand_NN, because_IN of_IN their_PRP$ chain_NN structure_NN, RNNs_NNP are_VBP better_RB at_IN keeping_VBG track_NN of_IN short-distance_NN rather_RB than_IN long-distance_JJ dependencies_NNS and_CC thus_RB we_PRP can_MD expect_VB tokens_NNS in_IN positions_NNS closer_VBP to_TO the_DT end_NN of_IN the_DT sentence_NN to_TO be_VB more_RBR important_JJ. On_IN the_DT other_JJ hand_NN, in_IN English_NNP the_DT information_NN structure_NN of_IN a_DT single_JJ sentence_NN is_VBZ expressed_VBN via_IN linear_JJ ordering_NN: The_DT topic_NN of_IN a_DT sentence_NN appears_VBZ sentence-initially_RB, and_CC the_DT comment_NN follows_VBZ. In_IN the_DT context_NN of_IN other_JJ text_JJ types_NNS such_JJ as_IN dialog_NN or_CC multi-sentence_NN narrative_JJ structure_NN, we_PRP would_MD expect_VB comment_NN to_TO often_RB be_VB more_RBR important_JJ than_IN topic_NN as_IN comment_NN will_MD often_RB contain_VB new_JJ information_NN in_IN these_DT cases_NNS. In_IN our_PRP$ setting_NN of_IN image_NN captions_NNS, however_RB, sentences_NNS are_VBP not_RB part_NN of_IN a_DT larger_JJR discourse_NN; it_PRP is_VBZ sentence-initial_JJ material_NN that_WDT typically_RB contains_VBZ the_DT most_RBS important_JJ objects_NNS depicted_VBN in_IN the_DT image_NN (e.g._JJ, two_CD zebras_NNS are_VBP grazing_VBG in_IN tall_JJ grass_NN on_IN a_DT savannah_NN). Thus_RB, for_IN the_DT task_NN of_IN predicting_VBG features_NNS of_IN the_DT visual_JJ scene_NN, it_PRP would_MD be_VB advantageous_JJ to_TO detect_VB the_DT topic_NN of_IN the_DT sentence_NN and_CC up-weight_JJ its_PRP$ importance_NN in_IN the_DT final_JJ meaning_NN representation_NN. Figure_NN 9_CD appears_VBZ to_TO support_VB this_DT hypothesis_NN and_CC the_DT network_NN does_VBZ learn_VB to_TO pay_VB more_JJR attention_NN to_TO words_NNS appearing_VBG sentence-initially_RB. This_DT effect_NN seems_VBZ to_TO be_VB to_TO some_DT extent_NN mixed_JJ with_IN the_DT recency_NN bias_NN of_IN RNNs_NNP as_IN perhaps_RB indicated_VBN by_IN the_DT relatively_RB high_JJ coefficient_NN of_IN the_DT last_JJ position_NN for_IN Visual_NNP.
 4.4_CD Lexical_JJ versus_NN Abstract_NNP Contexts_NNP
 We_PRP would_MD like_VB to_TO further_JJ analyze_VB the_DT kinds_NNS of_IN linguistic_JJ features_NNS that_IN the_DT hidden_JJ dimensions_NNS of_IN RNNs_NNP encode_NN. Previous_JJ work_NN (Karpathy_NNP, Johnson_NNP, and_CC Li_NNP, 2016_CD; Li_NNP et_FW al._NN, 2016b_CD) has_VBZ shown_VBN that_IN, in_IN response_NN to_TO the_DT task_NN the_DT networks_NNS are_VBP trained_VBN for_IN, individual_JJ dimensions_NNS in_IN the_DT hidden_JJ layers_NNS of_IN RNNs_NNP can_MD become_VB specialized_JJ in_IN responding_VBG to_TO certain_JJ types_NNS of_IN triggers_NNS, including_VBG the_DT tokens_NNS or_CC token_VB types_NNS at_IN each_DT time_NN step_NN, as_RB well_RB as_IN the_DT preceding_VBG context_NN of_IN each_DT token_NN in_IN the_DT input_NN sentence_NN.
 Here_RB we_PRP perform_VBP a_DT further_JJ comparison_NN between_IN the_DT models_NNS based_VBN on_IN the_DT hypothesis_NN that_IN, due_JJ to_TO their_PRP$ different_JJ objectives_NNS, the_DT activations_NNS of_IN the_DT dimensions_NNS of_IN the_DT last_JJ hidden_JJ layer_NN of_IN Visual_NNP are_VBP more_RBR characterized_JJ by_IN semantic_JJ relations_NNS within_IN contexts_NN, whereas_IN the_DT hidden_JJ dimensions_NNS in_IN Textual_NNP and_CC LM_NNP are_VBP more_RBR focused_JJ on_IN extracting_VBG syntactic_JJ patterns_NNS. In_IN order_NN to_TO quantitatively_RB test_VB this_DT hypothesis_NN, we_PRP measure_VBP the_DT strength_NN of_IN association_NN between_IN activations_NNS of_IN hidden_JJ dimensions_NNS and_CC either_DT lexical_JJ (token_JJ n-grams_NNS) or_CC structural_JJ (dependency_JJ label_RB n-grams_NNS) types_NNS of_IN context_NN.
 For_IN each_DT pathway_NN, we_PRP define_VBP Ai_NNP as_IN a_DT discrete_JJ random_NN variable_JJ corresponding_NN to_TO a_DT binned_VBN activation_NN over_IN time_NN steps_NNS at_IN hidden_JJ dimension_NN i_NN, and_CC C_NNP as_IN a_DT discrete_JJ random_NN variable_JJ indicating_VBG the_DT context_NN (where_WRB C_NNP can_MD be_VB of_IN type_JJ ¡°word_NNP trigram¡±_NN or_CC ¡°dependency_NN label_NN bigram_NN, ¡±_NN for_IN example_NN). The_DT strength_NN of_IN association_NN between_IN Ai_NNP and_CC C_NNP can_MD be_VB measured_VBN by_IN their_PRP$ mutual_JJ information_NN.
 Similarly_RB to_TO Li_NNP et_FW al_NN. (2016b_CD), the_DT activation_NN value_NN distributions_NNS are_VBP discretized_VBN into_IN percentile_JJ bins_NNS per_IN dimension_NN, such_JJ that_IN each_DT bin_NN contains_VBZ 5_CD% of_IN the_DT marginal_JJ density_NN. For_IN context_JJ types_NNS, we_PRP used_VBD unigrams_NNS, bigrams_NNS, and_CC trigrams_NNS of_IN both_DT dependency_NN labels_NNS and_CC words_NNS. Figure_NN 10_CD shows_VBZ the_DT distributions_NNS of_IN the_DT mutual_JJ information_NN scores_NNS for_IN the_DT three_CD networks_NNS and_CC the_DT six_CD context_NN types_NNS. Note_NN that_IN the_DT scores_NNS are_VBP not_RB easily_RB comparable_JJ between_IN context_NN types_NNS, because_IN of_IN the_DT different_JJ support_NN of_IN the_DT distributions_NNS; they_PRP are_VBP, however_RB, comparable_JJ across_IN the_DT networks_NNS. The_DT figure_NN shows_VBZ LM_NNP and_CC Textual_NNP as_IN being_VBG very_RB similar_JJ, whereas_JJ Visual_NNP exhibits_VBZ a_DT different_JJ distribution_NN. We_PRP next_JJ compare_VBP the_DT models_NNS '_POS scores_NNS pairwise_VBP to_TO pinpoint_VB the_DT nature_NN of_IN the_DT differences_NNS.
 We_PRP use_VBP the_DT notation_NN MILMCMICLM_NNP, MITCMICT_NNP, and_CC MIVCMICV_NNP to_TO denote_VB the_DT median_JJ mutual_JJ information_NN score_NN over_IN all_DT dimensions_NNS of_IN LM_NNP, Textual_NNP, and_CC Visual_NNP, respectively_RB, when_WRB considering_VBG context_JJ C._NNP We_PRP then_RB compute_VBP log_JJ ratios_NNS log_NN (MITC/MIVC_NNP) log_NN (MICT/MICV_NNP) and_CC log_NN (MILMC/MITC_NNP) log_NN (MICLM/MICT_NNP) for_IN all_DT six_CD context_JJ types_NNS C._NNP In_IN order_NN to_TO quantify_VB variability_NN we_PRP bootstrap_VBP this_DT statistic_JJ with_IN 5,000_CD replicates_NNS. Figure_NN 11_CD shows_VBZ the_DT resulting_VBG bootstrap_NN distributions_NNS for_IN unigram_JJ, bigram_NN, and_CC trigram_NN contexts_NN, in_IN the_DT word_NN and_CC dependency_NN conditions_NNS.
 Bootstrap_NNP distributions_NNS of_IN log_JJ ratios_NNS of_IN median_JJ mutual_JJ information_NN scores_NNS for_IN word_NN and_CC dependency_NN contexts_NN. Left_VBN: Textual_JJ vs_NN Visual_NNP; right_NN: LM_NNP vs_VBP Textual_NNP.
 The_DT clear_JJ pattern_NN is_VBZ that_IN for_IN Textual_NNP versus_NN Visual_NNP, the_DT log_NN ratios_NNS are_VBP much_JJ higher_JJR in_IN the_DT case_NN of_IN the_DT dependency_NN contexts_NN, with_IN no_DT overlap_NN between_IN the_DT bootstrap_NN distributions_NNS. Thus_RB, in_IN general_JJ, the_DT size_NN of_IN the_DT relative_JJ difference_NN between_IN Textual_NNP and_CC Visual_NNP median_JJ mutual_JJ information_NN score_NN is_VBZ much_RB more_RBR pronounced_JJ for_IN dependency_NN context_NN types_NNS. This_DT suggests_VBZ that_WDT features_VBZ that_WDT are_VBP encoded_VBN by_IN the_DT hidden_JJ dimensions_NNS of_IN the_DT models_NNS are_VBP indeed_RB different_JJ, and_CC that_IN the_DT features_NNS encoded_VBN by_IN Textual_NNP are_VBP more_RBR associated_JJ with_IN syntactic_JJ constructions_NNS than_IN in_IN the_DT case_NN of_IN Visual_NNP. In_IN contrast_NN, when_WRB comparing_VBG LM_NNP with_IN Textual_NNP, the_DT difference_NN between_IN context_JJ types_NNS is_VBZ much_RB less_RBR pronounced_JJ, with_IN distributions_NNS overlapping_VBG. Though_IN the_DT difference_NN is_VBZ small_JJ, it_PRP goes_VBZ in_IN the_DT direction_NN of_IN the_DT dimensions_NNS of_IN the_DT Textualmodel_NNP showing_VBG higher_JJR sensitivity_NN towards_NNS dependency_NN contexts_NN.
 The_DT mutual_JJ information_NN scores_NNS can_MD be_VB used_VBN to_TO pinpoint_VB specific_JJ dimensions_NNS of_IN the_DT hidden_JJ activation_NN vectors_NNS that_WDT are_VBP strongly_RB associated_VBN with_IN a_DT particular_JJ type_NN of_IN context_NN. Table_JJ 2lists_CD for_IN each_DT network_NN the_DT dimension_NN with_IN the_DT highest_JJS mutual_JJ information_NN score_NN with_IN respect_NN to_TO the_DT dependency_NN trigram_NN context_NN type_NN, together_RB with_IN the_DT top_JJ five_CD contexts_NNS where_WRB these_DT dimensions_NNS carry_VBP the_DT highest_JJS value_NN. In_IN spite_NN of_IN the_DT quantitative_JJ difference_NN between_IN the_DT networks_NNS discussed_VBN earlier_RBR, the_DT dimensions_NNS that_WDT come_VBP up_RP top_JJ seem_NN to_TO be_VB capturing_VBG something_NN quite_RB similar_JJ for_IN the_DT three_CD networks_NNS: (a_DT part_NN of_IN) a_DT construction_NN with_IN an_DT animate_JJ root_NN or_CC subject_JJ modified_VBN by_IN a_DT participle_NN or_CC a_DT prepositional_JJ phrase_NN, though_IN this_DT is_VBZ somewhat_RB less_JJR clean-cut_JJ for_IN the_DT Visual_NNP pathway_NN where_WRB only_RB two_CD out_IN of_IN five_CD top_JJ contexts_NN clearly_RB conform_NN to_TO this_DT pattern_NN. Other_JJ interesting_VBG templates_NNS can_MD be_VB found_VBN by_IN visual_JJ inspection_NN of_IN the_DT contexts_NN where_WRB high-scoring_NN dimensions_NNS are_VBP active_JJ; for_IN example_NN, dimension_NN 324_CD of_IN LM_NNP is_VBZ high_JJ for_IN word_NN bigram_NN contexts_NN including_VBG people_NNS preparing_VBG, gets_VBZ ready_JJ, man_NN preparing_NN, woman_NN preparing_NN, teenager_NN preparing_NN. </discussion_NN>




 <conclusion_NN> The_DT goal_NN of_IN our_PRP$ article_NN is_VBZ to_TO propose_VB novel_JJ methods_NNS for_IN the_DT analysis_NN of_IN the_DT encoding_NN of_IN linguistic_JJ knowledge_NN in_IN RNNs_NNP trained_VBD on_IN language_NN tasks_NNS. We_PRP focused_VBD on_IN developing_VBG quantitative_JJ methods_NNS to_TO measure_VB the_DT importance_NN of_IN different_JJ kinds_NNS of_IN words_NNS for_IN the_DT performance_NN of_IN such_JJ models_NNS. Furthermore_RB, we_PRP proposed_VBD techniques_NNS to_TO explore_VB what_WP kinds_NNS of_IN linguistic_JJ features_NNS the_DT models_NNS learn_VBP to_TO exploit_VB beyond_IN lexical_JJ cues_NNS.
 Using_VBG the_DT Imaginet_NNP model_NN as_IN our_PRP$ case_NN study_NN, our_PRP$ analyses_NNS of_IN the_DT hidden_JJ activation_NN patterns_NNS show_VBP that_IN the_DT Visual_NNP model_NN learns_VBZ an_DT abstract_JJ representation_NN of_IN the_DT information_NN structure_NN of_IN a_DT single_JJ sentence_NN in_IN the_DT language_NN, and_CC pays_NNS selective_JJ attention_NN to_TO lexical_JJ categories_NNS and_CC grammatical_JJ functions_NNS that_WDT carry_VBP semantic_JJ information_NN. In_IN contrast_NN, the_DT language_NN model_NN Textual_NNP is_VBZ sensitive_JJ to_TO features_NNS of_IN a_DT more_RBR syntactic_JJ nature_NN. We_PRP have_VBP also_RB shown_VBN that_IN each_DT network_NN contains_VBZ specialized_VBN units_NNS that_WDT are_VBP tuned_VBN to_TO both_DT lexical_JJ and_CC structural_JJ patterns_NNS that_WDT are_VBP useful_JJ for_IN the_DT task_NN at_IN hand_NN.
 5.1_CD Generalizing_VBG to_TO Other_JJ Architectures_NNS
 For_IN other_JJ RNN_NNP architectures_NNS such_JJ as_IN LSTMs_NNP and_CC their_PRP$ bi-directional_JJ variants_NNS, measuring_VBG the_DT contribution_NN of_IN tokens_NNS to_TO their_PRP$ predictions_NNS (or_CC the_DT omission_NN scores_NNS) can_MD be_VB straightforwardly_RB computed_VBN using_VBG their_PRP$ hidden_NN state_NN at_IN the_DT last_JJ time_NN step_NN used_VBN for_IN prediction_NN. Furthermore_RB, the_DT technique_NN can_MD be_VB applied_VBN in_IN general_JJ to_TO other_JJ architectures_NNS that_WDT map_VBP variable-length_JJ linguistic_JJ expressions_NNS to_TO the_DT same_JJ fixed_JJ dimensional_JJ space_NN and_CC perform_VB predictions_NNS based_VBN on_IN these_DT embeddings_NNS. This_DT includes_VBZ tree-structured_JJ RNN_NNP models_NNS such_JJ as_IN the_DT Tree-LSTM_NNP introduced_VBD in_IN Tai_NNP, Socher_NNP, and_CC Manning_NNP (2015_CD), or_CC the_DT CNN_NNP architecture_NN of_IN Kim_NNP (2014_CD) for_IN sentence_NN classification_NN. However_RB, the_DT presented_VBN analysis_NN and_CC results_NNS regarding_VBG word_NN positions_NNS can_MD only_RB be_VB meaningful_JJ for_IN RNNs_NNP as_IN they_PRP compute_VBP their_PRP$ representations_NNS sequentially_RB and_CC are_VBP not_RB limited_VBN by_IN fixed_VBN window_NN sizes_NNS.
 A_DT limitation_NN of_IN the_DT generalizability_NN of_IN our_PRP$ analysis_NN is_VBZ that_IN in_IN the_DT case_NN of_IN bi-directional_JJ architectures_NNS, the_DT interpretation_NN of_IN the_DT features_NNS extracted_VBN by_IN the_DT RNNs_NNP that_WDT process_VBD the_DT input_NN tokens_VBZ in_IN the_DT reversed_JJ order_NN might_MD be_VB hard_RB from_IN a_DT linguistic_JJ point_NN of_IN view_NN.
 5.2_CD Future_JJ Directions_NNS
 In_IN the_DT future_NN we_PRP would_MD like_VB to_TO apply_VB the_DT techniques_NNS introduced_VBN in_IN this_DT article_NN to_TO analyze_VB the_DT encoding_NN of_IN linguistic_JJ form_NN and_CC function_NN of_IN recurrent_JJ neural_JJ models_NNS trained_VBN on_IN different_JJ objectives_NNS, such_JJ as_IN neural_JJ machine_NN translation_NN systems_NNS (Sutskever_NNP, Vinyals_NNP, and_CC Le_NNP, 2014_CD) or_CC the_DT purely_RB distributional_JJ sentence_NN embedding_VBG system_NN of_IN Kiros_NNP et_FW al_NN. (2015_CD). A_DT number_NN of_IN recurrent_JJ neural_JJ models_NNS rely_VBP on_IN a_DT so-called_JJ attention_NN mechanism_NN, first_RB introduced_VBN by_IN Bahdanau_NNP, Cho_NNP, and_CC Bengio_NNP (2015_CD) under_IN the_DT name_NN of_IN soft_JJ alignment_NN. In_IN these_DT networks_NNS attention_NN is_VBZ explicitly_RB represented_VBN, and_CC it_PRP would_MD be_VB interesting_JJ to_TO see_VB how_WRB our_PRP$ method_NN of_IN discovering_VBG implicit_JJ attention_NN, the_DT omission_NN score_NN, compares_VBZ. For_IN future_JJ work_NN we_PRP also_RB propose_VBP to_TO collect_VB data_NNS where_WRB humans_NNS assess_VBP the_DT importance_NN of_IN each_DT word_NN in_IN a_DT sentence_NN and_CC explore_VB the_DT relationship_NN between_IN omission_NN scores_NNS for_IN various_JJ models_NNS and_CC human_JJ annotations_NNS. Finally_RB, one_CD of_IN the_DT benefits_NNS of_IN understanding_VBG how_WRB linguistic_JJ form_NN and_CC function_NN is_VBZ represented_VBN in_IN RNNs_NNP is_VBZ that_IN it_PRP can_MD provide_VB insight_RB into_IN how_WRB to_TO improve_VB systems_NNS. We_PRP plan_VBP to_TO draw_VB on_IN lessons_NNS learned_VBN from_IN our_PRP$ analyses_NNS in_IN order_NN to_TO develop_VB models_NNS with_IN better_JJR general-purpose_JJ sentence_NN representations_NNS. </conclusion_NN>
